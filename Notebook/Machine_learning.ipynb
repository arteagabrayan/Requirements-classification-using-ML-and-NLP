{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81368f4f",
   "metadata": {
    "id": "81368f4f"
   },
   "source": [
    "- **Requirements classification in the customer service area for software companies using Machine Learning and Natural Language Processing**\n",
    "\n",
    "The dataset that will be worked on is called \"TCC.xlsx\" and contains the information of the requirements, requests and petitions presented to the company SIGMA Ingenier√≠a S.A of Manizales in the technical support area. \n",
    "\n",
    "The fields that will be taken into account for this work will be \"description\" and \"category\", the idea is to find the best performance technique in the classification of descriptions to implement in the company and perform the automatic classification of future requirements, it is intended that Through the predicted category, the protocols of solution to the request presented by the client are provided to offer a better quality in the response and also, reduce the time in the response by the service area and technical support of the company towards the client. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00f6332",
   "metadata": {
    "id": "c00f6332"
   },
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Y7FdNdVMbNWy",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y7FdNdVMbNWy",
    "outputId": "05eb1e78-ab02-4438-a49b-0951768d2d40",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Classification Methods\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#NLP\n",
    "import nltk \n",
    "nltk.download('stopwords') \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.corpus import stopwords \n",
    "import spacy \n",
    "import en_core_web_sm\n",
    "\n",
    "#Metrics\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from yellowbrick.classifier import ClassificationReport \n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#Tools\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import model_selection\n",
    "from scipy.sparse import csr_matrix \n",
    "import string \n",
    "import time as tm\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b57a999",
   "metadata": {
    "id": "8b57a999"
   },
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8f3a2c",
   "metadata": {
    "id": "2b8f3a2c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def lemmatizer(text):  \n",
    "    doc = nlp(text)\n",
    "    return ' '.join([word.lemma_ for word in doc]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cc9b56",
   "metadata": {
    "id": "f7cc9b56"
   },
   "outputs": [],
   "source": [
    "def classifier_metrics(X_train,X_test,y_train,y_test):    \n",
    "    def metrics(model):\n",
    "        print(\"\\nHold-Out in process...\")\n",
    "        start_time = tm.time()\n",
    "        model.fit(X_train, y_train) \n",
    "        TIME = tm.time() - start_time \n",
    "        print(\"Time, Training: {0:.4f} [seconds]\".format(TIME))\n",
    "        start_time = tm.time()\n",
    "        y_pred = model.predict(X_test)\n",
    "        TIME = tm.time() - start_time \n",
    "        print(\"Time, Prediction: {0:.4f} [seconds]\".format(TIME))\n",
    "        accuracy_s  = accuracy_score(y_test,y_pred) \n",
    "        f1_s        = f1_score(y_test,y_pred,average='weighted')\n",
    "        recall_s    = recall_score(y_test,y_pred,average='weighted')\n",
    "        precision_s = precision_score(y_test,y_pred,average='weighted')\n",
    "        print('accuracy_score: {0:.4f}'.format(accuracy_s))\n",
    "        print('f1_score: {0:.4f}'.format(f1_s))\n",
    "        print('recall_score: {0:.4f}'.format(recall_s))\n",
    "        print('precision_score: {0:.4f}'.format(precision_s))\n",
    "        \n",
    "        print('\\nCross-Validation in process...')\n",
    "        start_time = tm.time() \n",
    "        kfold = model_selection.KFold(n_splits=10)\n",
    "        y_CV = np.concatenate((y_train,y_test))\n",
    "        if \"GaussianNB\" in str(name) or \"LinearDiscriminantAnalysis\" in str(name):\n",
    "            X_CV = np.concatenate((X_train,X_test))\n",
    "            cv_results = np.array(model_selection.cross_val_score(model, X_CV, y_CV, cv=kfold, scoring='accuracy', n_jobs=-1))\n",
    "        else:\n",
    "            X_CV = np.concatenate((X_train.toarray(),X_test.toarray()))\n",
    "            X_CV = csr_matrix(X_CV)\n",
    "            cv_results = np.array(model_selection.cross_val_score(model, X_CV, y_CV, cv=kfold, scoring='accuracy', n_jobs=-1))\n",
    "        \n",
    "        cv_results = cv_results[np.logical_not(np.isnan(cv_results))] \n",
    "        TIME = tm.time() - start_time \n",
    "        print(\"Time, CV: {0:.4f} [seconds]\".format(TIME))\n",
    "        print('CV: {0:.4f} {1:.4f}'.format(cv_results.mean(),cv_results.std()))\n",
    "\n",
    "    for name in classifiers:\n",
    "        print (\"---------------------------------------------------------------------------------\\n\") \n",
    "        print(str(name))\n",
    "        if \"GaussianNB\" in str(name) or \"LinearDiscriminantAnalysis\" in str(name):\n",
    "            X_train=csr_matrix(X_train) \n",
    "            X_test =csr_matrix(X_test) \n",
    "            X_train=X_train.toarray() \n",
    "            X_test=X_test.toarray() \n",
    "        else:\n",
    "            X_train=csr_matrix(X_train)\n",
    "            X_test=csr_matrix(X_test)\n",
    "            \n",
    "        metrics(name)\n",
    "        print()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62aed031",
   "metadata": {
    "id": "62aed031"
   },
   "outputs": [],
   "source": [
    "path_figures = \"../images\"\n",
    "if not os.path.exists(path_figures):\n",
    "    os.makedirs(path_figures)\n",
    "\n",
    "# Classification report\n",
    "def CR_viz():\n",
    "    ax = plt.figure(figsize=(15,20)) \n",
    "    visualizer = ClassificationReport(model_selected, classes=classes, support=True,  \n",
    "                                      cmap='Blues', title=\"Classification Report - \"+model_name)\n",
    "    visualizer.fit(X_train, y_train)   \n",
    "    visualizer.score(X_test, y_test)      \n",
    "    visualizer.poof()\n",
    "    ax.show()\n",
    "    ax.savefig(path_figures+\"/\"+model_name+\"_CR\"+\".pdf\", bbox_inches = \"tight\") \n",
    "\n",
    "# Confusion matrix\n",
    "def CM_viz():\n",
    "    model_selected.fit(X_train, y_train) \n",
    "    y_pred = model_selected.predict(X_test) \n",
    "    conf = confusion_matrix(y_test, y_pred) \n",
    "    plt.figure(figsize=(42 , 42)) \n",
    "    annot_kws={'fontsize':20, 'verticalalignment':'center' }\n",
    "    ax = sns.heatmap(conf, annot=True, cmap='Blues',fmt = 'd',annot_kws= annot_kws, xticklabels=np.unique(y_test), yticklabels=np.unique(y_test)) \n",
    "    ax.set(title=\"Confusion Matrix with labels\", xlabel=\"Predicted Values\", ylabel=\"Actual Values\")\n",
    "    sns.set(font_scale=2)\n",
    "    plt.title(\"Confusion Matrix - \"+model_name, fontsize = 35)\n",
    "    plt.xlabel(\"Predicted Values\", fontsize = 35)\n",
    "    plt.ylabel(\"Actual Values\", fontsize = 35)\n",
    "    plt.savefig(path_figures+\"/\"+model_name+\"_CM\"+\".pdf\", bbox_inches = \"tight\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fb6e59",
   "metadata": {
    "id": "a8fb6e59"
   },
   "source": [
    "# Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mQdXPilDs7Cj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "mQdXPilDs7Cj",
    "outputId": "70e31e3e-e25c-4fce-b97c-3c391b114650",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loading Dataset\n",
    "filename = '../Data/TCC.xlsx'\n",
    "DataSet0 = pd.read_excel(os.path.join(filename), engine='openpyxl')\n",
    "DataSet0.shape "
   ]
  },
  {
   "cell_type": "raw",
   "id": "8841d09a",
   "metadata": {
    "id": "8841d09a"
   },
   "source": [
    "'''\n",
    "ax = DataSet0['category'].value_counts().plot(kind='bar', figsize=(10, 6), fontsize=14)\n",
    "ax.set_title('Classes', size=20, pad=30)\n",
    "ax.set_ylabel('Frecuency', fontsize=14)\n",
    "print(f'Number of categories {len(DataSet0[\"category\"].value_counts())}')\n",
    "print(DataSet0[\"category\"].value_counts())\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65853643",
   "metadata": {
    "id": "65853643"
   },
   "source": [
    "# Machine learning application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc2a335",
   "metadata": {
    "id": "bbc2a335"
   },
   "source": [
    "## 1. Original Dataset (OD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e98d5f3",
   "metadata": {
    "id": "6e98d5f3"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "'X' and 'y' are defined, 'X' will be in charge of containing the characteristics of the dataset that for this case\n",
    "is the description that will define the category to which it belongs and 'y' contains the values of the labels, \n",
    "in this case of the possible categories defined.\n",
    "'''\n",
    "\n",
    "X = DataSet0['description'] \n",
    "y = DataSet0['category'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa2170f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6aa2170f",
    "outputId": "feb613d3-fede-45d2-acfb-4eac3091d436",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "The data for training and validation is defined, the training data will represent 80% of all the data \n",
    "and the validation data the remaining 20%\n",
    "'''\n",
    "X_train, X_test, y_train, y_test = train_test_split(X , y , test_size=0.20, random_state=8, stratify=y)\n",
    "\n",
    "print(X_train.shape,X_test.shape,y_train.shape,y_test.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb728d9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0eb728d9",
    "outputId": "22f3a141-2581-4d86-901a-aa1c5b362800",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Bearing in mind that the problem worked consists of text, it is necessary to transform them and prepare them for \n",
    "later use, in this case, each of the words contained in the description will be encoded in floating point values \n",
    "for use in machine learning algorithms, this process is also known as feature extraction or vectorization \n",
    "using the TfidfVectorizer library\n",
    "'''\n",
    "\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', ngram_range=(1, 2), stop_words=stopwords.words(\"english\"))\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "\n",
    "print(X_train.shape,X_test.shape,y_train.shape,y_test.shape) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f4d0d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ML Models definition\n",
    "classifiers=[svm.SVC(), \n",
    "             ExtraTreesClassifier(n_jobs=-1), \n",
    "             RandomForestClassifier(n_jobs=-1),\n",
    "             LogisticRegression(solver='liblinear'),\n",
    "             DecisionTreeClassifier(),\n",
    "             LinearDiscriminantAnalysis(),\n",
    "             GaussianNB(),\n",
    "             KNeighborsClassifier()\n",
    "            ] \n",
    "\n",
    "#Deploy aggregate metrics \n",
    "classifier_metrics(X_train,X_test,y_train,y_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fa3d66",
   "metadata": {
    "id": "b8fa3d66"
   },
   "source": [
    "## 2. Dataset with Preprocessing (DP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7306d7",
   "metadata": {
    "id": "6e7306d7"
   },
   "outputs": [],
   "source": [
    "# Convert texts to lowercase\n",
    "DataSet0['description'] = DataSet0['description'].str.lower()\n",
    "DataSet0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c14bf3",
   "metadata": {
    "id": "50c14bf3"
   },
   "outputs": [],
   "source": [
    "# Remove punctuation marks\n",
    "punct = string.punctuation\n",
    "\n",
    "for c in punct:\n",
    "    for fila in range(len(DataSet0)):\n",
    "        DataSet0['description'][fila] = DataSet0['description'][fila].replace(c, \" \")\n",
    "DataSet0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240984c4",
   "metadata": {
    "id": "240984c4"
   },
   "outputs": [],
   "source": [
    "# Apply stemming to the description field\n",
    "DataSet0['description'] = DataSet0['description'].apply(lambda x: lemmatizer(x)) \n",
    "DataSet0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d09359a",
   "metadata": {
    "id": "0d09359a"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "'X' and 'y' are defined, 'X' will be in charge of containing the characteristics of the dataset that for this case\n",
    "is the description that will define the category to which it belongs and 'y' contains the values of the labels, \n",
    "in this case of the possible categories defined.\n",
    "'''\n",
    "\n",
    "X = DataSet0['description'] \n",
    "y = DataSet0['category'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46272f9b",
   "metadata": {
    "id": "46272f9b"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "The data for training and validation is defined, the training data will represent 80% of all the data \n",
    "and the validation data the remaining 20%\n",
    "'''\n",
    "X_train, X_test, y_train, y_test = train_test_split(X , y , test_size=0.20, random_state=8, stratify=y)\n",
    "\n",
    "print(X_train.shape,X_test.shape,y_train.shape,y_test.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a02f82",
   "metadata": {
    "id": "35a02f82"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Bearing in mind that the problem worked consists of text, it is necessary to transform them and prepare them for \n",
    "later use, in this case, each of the words contained in the description will be encoded in floating point values \n",
    "for use in machine learning algorithms, this process is also known as feature extraction or vectorization \n",
    "using the TfidfVectorizer library\n",
    "'''\n",
    "\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', ngram_range=(1, 2), stop_words=stopwords.words(\"english\"))\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "\n",
    "print(X_train.shape,X_test.shape,y_train.shape,y_test.shape) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2302c899",
   "metadata": {
    "id": "2302c899",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ML Models definition\n",
    "classifiers=[svm.SVC(), ExtraTreesClassifier(n_jobs=-1), RandomForestClassifier(n_jobs=-1)] \n",
    "\n",
    "#Deploy aggregate metrics \n",
    "classifier_metrics(X_train,X_test,y_train,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33e7f47",
   "metadata": {
    "id": "b33e7f47"
   },
   "source": [
    "## 3. Dataset with Preprocessing and Balancing (DPB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8711c818",
   "metadata": {
    "id": "8711c818"
   },
   "outputs": [],
   "source": [
    "# Convert texts to lowercase\n",
    "DataSet0['description'] = DataSet0['description'].str.lower()\n",
    "DataSet0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3498f3",
   "metadata": {
    "id": "bf3498f3"
   },
   "outputs": [],
   "source": [
    "# Remove punctuation marks\n",
    "punct = string.punctuation\n",
    "\n",
    "for c in punct:\n",
    "    for fila in range(len(DataSet0)):\n",
    "        DataSet0['description'][fila] = DataSet0['description'][fila].replace(c, \" \")\n",
    "DataSet0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6fc8b9",
   "metadata": {
    "id": "6d6fc8b9"
   },
   "outputs": [],
   "source": [
    "# Apply stemming to the description field\n",
    "DataSet0['description'] = DataSet0['description'].apply(lambda x: lemmatizer(x)) \n",
    "DataSet0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac91420",
   "metadata": {
    "id": "cac91420"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "'X' and 'y' are defined, 'X' will be in charge of containing the characteristics of the dataset that for this case\n",
    "is the description that will define the category to which it belongs and 'y' contains the values of the labels, \n",
    "in this case of the possible categories defined.\n",
    "'''\n",
    "\n",
    "X = DataSet0['description'] \n",
    "y = DataSet0['category'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf6a84d",
   "metadata": {
    "id": "ebf6a84d"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Bearing in mind that the problem worked consists of text, it is necessary to transform them and prepare them for \n",
    "later use, in this case, each of the words contained in the description will be encoded in floating point values \n",
    "for use in machine learning algorithms, this process is also known as feature extraction or vectorization \n",
    "using the TfidfVectorizer library\n",
    "'''\n",
    "\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', ngram_range=(1, 2), stop_words=stopwords.words(\"english\"))\n",
    "X = vectorizer.fit_transform(X) \n",
    "\n",
    "print(X.shape,y.shape) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553154d8",
   "metadata": {
    "id": "553154d8"
   },
   "outputs": [],
   "source": [
    "sm = SMOTE(random_state=42)\n",
    "\n",
    "X_sm, y_sm = sm.fit_resample(X, y) \n",
    "\n",
    "print(f'''Shape of X before SMOTE: {X.shape}\n",
    "Shape of X after SMOTE: {X_sm.shape}''')\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3f4ee68d",
   "metadata": {
    "id": "3f4ee68d"
   },
   "source": [
    "'''\n",
    "print('\\nBalance of positive and negative classes (%):')\n",
    "y_sm.value_counts(normalize=True) * 100\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c987969b",
   "metadata": {
    "id": "c987969b"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "The data for training and validation is defined, the training data will represent 80% of all the data \n",
    "and the validation data the remaining 20%\n",
    "'''\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_sm, y_sm, test_size=0.20, random_state=8, stratify=y_sm)\n",
    "\n",
    "print(X_train.shape,X_test.shape,y_train.shape,y_test.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc52a86",
   "metadata": {
    "id": "8fc52a86",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ML Models definition\n",
    "classifiers=[svm.SVC(), ExtraTreesClassifier(n_jobs=-1), RandomForestClassifier(n_jobs=-1)] \n",
    "\n",
    "#Deploy aggregate metrics \n",
    "classifier_metrics(X_train,X_test,y_train,y_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616077d2",
   "metadata": {
    "id": "616077d2"
   },
   "source": [
    "## 4. Dataset with Preprocessing and Balancing, Optimization of parameters (DPBO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8698e05",
   "metadata": {
    "id": "d8698e05"
   },
   "outputs": [],
   "source": [
    "# Convert texts to lowercase\n",
    "DataSet0['description'] = DataSet0['description'].str.lower()\n",
    "DataSet0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a8cb57",
   "metadata": {
    "id": "98a8cb57"
   },
   "outputs": [],
   "source": [
    "# Remove punctuation marks\n",
    "punct = string.punctuation\n",
    "\n",
    "for c in punct:\n",
    "    for fila in range(len(DataSet0)):\n",
    "        DataSet0['description'][fila] = DataSet0['description'][fila].replace(c, \" \")\n",
    "DataSet0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd664f2d",
   "metadata": {
    "id": "bd664f2d"
   },
   "outputs": [],
   "source": [
    "# Apply stemming to the description field\n",
    "DataSet0['description'] = DataSet0['description'].apply(lambda x: lemmatizer(x)) \n",
    "DataSet0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525895a7",
   "metadata": {
    "id": "525895a7"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "'X' and 'y' are defined, 'X' will be in charge of containing the characteristics of the dataset that for this case\n",
    "is the description that will define the category to which it belongs and 'y' contains the values of the labels, \n",
    "in this case of the possible categories defined.\n",
    "'''\n",
    "\n",
    "X = DataSet0['description'] \n",
    "y = DataSet0['category'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43efd1f",
   "metadata": {
    "id": "c43efd1f"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Bearing in mind that the problem worked consists of text, it is necessary to transform them and prepare them for \n",
    "later use, in this case, each of the words contained in the description will be encoded in floating point values \n",
    "for use in machine learning algorithms, this process is also known as feature extraction or vectorization \n",
    "using the TfidfVectorizer library\n",
    "'''\n",
    "\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', ngram_range=(1, 2), stop_words=stopwords.words(\"english\"))\n",
    "X = vectorizer.fit_transform(X) \n",
    "\n",
    "print(X.shape,y.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70291c7",
   "metadata": {
    "id": "d70291c7"
   },
   "outputs": [],
   "source": [
    "sm = SMOTE(random_state=42)\n",
    "\n",
    "X_sm, y_sm = sm.fit_resample(X, y) \n",
    "\n",
    "print(f'''Shape of X before SMOTE: {X.shape}\n",
    "Shape of X after SMOTE: {X_sm.shape}''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99026ca",
   "metadata": {
    "id": "d99026ca"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "The data for training and validation is defined, the training data will represent 80% of all the data \n",
    "and the validation data the remaining 20%\n",
    "'''\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_sm, y_sm, test_size=0.20, random_state=8, stratify=y_sm)\n",
    "\n",
    "print(X_train.shape,X_test.shape,y_train.shape,y_test.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7743ef",
   "metadata": {
    "id": "ee7743ef",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Models with the Parameters Optimized\n",
    "classifiers=[svm.SVC(C = 10, gamma = 1, kernel = 'rbf'), \n",
    "             ExtraTreesClassifier(min_samples_leaf = 1,min_samples_split= 3, n_estimators= 100, n_jobs=-1), \n",
    "             RandomForestClassifier(min_samples_leaf= 1, min_samples_split= 2, n_estimators= 800, n_jobs=-1)] \n",
    "\n",
    "#Deploy aggregate metrics \n",
    "classifier_metrics(X_train,X_test,y_train,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d435628",
   "metadata": {
    "id": "2d435628"
   },
   "source": [
    "### Confusion Matrix and Classification Report for the best model and the best results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0256ed91",
   "metadata": {
    "id": "0256ed91"
   },
   "outputs": [],
   "source": [
    "model_name = \"SVM\"\n",
    "model_selected = svm.SVC(C = 10, gamma = 1, kernel = 'rbf') \n",
    "classes = np.unique(y_test)\n",
    "\n",
    "visualization =[CR_viz(), CM_viz()] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a21061",
   "metadata": {
    "id": "e2a21061"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Machine learning A.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
